# ‚úÖ embedding_pipeline.py (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÜÏù¥ DB + FAISSÎßå ÏÇ¨Ïö©ÌïòÎäî Íµ¨Ï°∞)

import os
import json
import numpy as np
import faiss
import pymysql
import requests
import tiktoken
from dotenv import load_dotenv
from tqdm import tqdm

# ‚úÖ ÌôòÍ≤Ω ÏÑ§Ï†ï
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1/embeddings"
MODEL_NAME = "text-embedding-3-large"
TOKEN_LIMIT = 8192
FAISS_PATH = "normalized_faiss_index/valorant.index"
JSONL_PATH = "api_data_ko.jsonl"
# JSONL_PATH = "api_data_new.jsonl"
# JSONL_PATH = "integrated_data.jsonl"
# JSONL_PATH = "wiki_processed_data.jsonl"

# ‚úÖ ÌÜ†ÌÅ∞ Ïπ¥Ïö¥ÌÑ∞
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text):
    return len(encoding.encode(text))

# ‚úÖ DB Ïó∞Í≤∞
conn = pymysql.connect(
    host=os.getenv("MYSQL_HOST"),
    user=os.getenv("MYSQL_USER"),
    password=os.getenv("MYSQL_PASSWORD"),
    # db=os.getenv("MYSQL_DB"),
    db=os.getenv("NEW_DB"),
    charset="utf8mb4"
)
cursor = conn.cursor()

# ‚úÖ Í∏∞Ï°¥ Ïù∏Îç±Ïä§ Î∂àÎü¨Ïò§Í∏∞ ÎòêÎäî ÏÉàÎ°ú ÎßåÎì§Í∏∞
if os.path.exists(FAISS_PATH):
    print("üì• Í∏∞Ï°¥ FAISS Ïù∏Îç±Ïä§ Î°úÎìú")
    faiss_index = faiss.read_index(FAISS_PATH)
else:
    print("üÜï ÏÉàÎ°úÏö¥ FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ± ÏòàÏ†ï")
    faiss_index = None

# ‚úÖ ÏûÑÎ≤†Îî© API

def add_embedding_to_faiss(faiss_index, emb):
    vec = np.array(emb, dtype=np.float32).reshape(1, -1)
    faiss.normalize_L2(vec)
    faiss_index.add(vec)
    return vec

def get_embeddings_batch(texts):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    data = {
        "model": MODEL_NAME,
        "input": texts
    }
    response = requests.post(EMBEDDING_URL, headers=headers, json=data)
    response.raise_for_status()
    return [item["embedding"] for item in response.json()["data"]]

# ‚úÖ ÏûÑÎ≤†Îî© Î∞è Ï†ÄÏû•
batch = []
batch_token_sum = 0
with open(JSONL_PATH, "r", encoding="utf-8") as f:
    for line in tqdm(f, desc="Processing"):
        item = json.loads(line.strip())
        title, content = item["title"], item["content"]
        full_text = f"{title}\n{content}"
        token_len = count_tokens(full_text)

        if token_len > TOKEN_LIMIT:
            print(f"‚ö†Ô∏è {title} ‚Üí {token_len} tokens (skip)")
            continue

        batch.append({"title": title, "content": content, "text": full_text})
        batch_token_sum += token_len

        if batch_token_sum > TOKEN_LIMIT:
            texts = [x["text"] for x in batch]
            embeddings = get_embeddings_batch(texts)

            if faiss_index is None:
                dim = len(embeddings[0])
                faiss_index = faiss.IndexFlatL2(dim)

            current_idx = faiss_index.ntotal

            for i, emb in enumerate(embeddings):
                vec = np.array(emb, dtype=np.float32).reshape(1, -1)
                faiss.normalize_L2(vec)
                faiss_index.add(vec)

                cursor.execute(
                    "INSERT INTO embeddings (vector_index, title, content, embedding) VALUES (%s, %s, %s, %s)",
                    (current_idx + i, batch[i]["title"], batch[i]["content"], json.dumps(emb))
                )

            conn.commit()
            batch, batch_token_sum = [], 0

# ‚úÖ ÎßàÏßÄÎßâ Î∞∞Ïπò
if batch:
    texts = [x["text"] for x in batch]
    embeddings = get_embeddings_batch(texts)

    if faiss_index is None:
        dim = len(embeddings[0])
        faiss_index = faiss.IndexFlatL2(dim)

    current_idx = faiss_index.ntotal

    for i, emb in enumerate(embeddings):
        vec = np.array(emb, dtype=np.float32).reshape(1, -1)
        faiss.normalize_L2(vec)
        faiss_index.add(vec)

        cursor.execute(
            "INSERT INTO embeddings (vector_index, title, content, embedding) VALUES (%s, %s, %s, %s)",
            (current_idx + i, batch[i]["title"], batch[i]["content"], json.dumps(emb))
        )

    conn.commit()

cursor.close()
conn.close()

# ‚úÖ Ïù∏Îç±Ïä§ Ï†ÄÏû•
os.makedirs(os.path.dirname(FAISS_PATH), exist_ok=True)
faiss.write_index(faiss_index, FAISS_PATH)
print("‚úÖ FAISS Ï†ÄÏû• ÏôÑÎ£å")
