import json
import os
import requests
import numpy as np
import faiss
import pymysql
from dotenv import load_dotenv
from tqdm import tqdm
import tiktoken  # âœ… ì •í™•í•œ í† í¬ë‚˜ì´ì €

# âœ… í™˜ê²½ ì„¤ì •
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1/embeddings"
MODEL_NAME = "text-embedding-3-large"
DIMENSION = 8192
TOKEN_LIMIT = 8192

# âœ… í† í° ì¹´ìš´íŒ… í•¨ìˆ˜
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text):
    return len(encoding.encode(text))

# âœ… DB ì—°ê²°
conn = pymysql.connect(
    host=os.getenv("MYSQL_HOST"),
    user=os.getenv("MYSQL_USER"),
    password=os.getenv("MYSQL_PASSWORD"),
    db=os.getenv("MYSQL_DB"),
    charset="utf8mb4"
)
cursor = conn.cursor()

# âœ… FAISS index (ì‹¤ì œ ì°¨ì›ìœ¼ë¡œ ì´ˆê¸°í™”)
# ì²« ë²ˆì§¸ ì„ë² ë”©ì„ ë°›ì€ í›„ ì˜¬ë°”ë¥¸ ì°¨ì›ìœ¼ë¡œ ì¬ì´ˆê¸°í™”
faiss_index = None
meta_data = []

# âœ… ì„ë² ë”© API
def get_embeddings_batch(texts):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_KEY}"
    }
    data = {
        "model": MODEL_NAME,
        "input": texts
    }
    
    print(f"ğŸ” API í˜¸ì¶œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ì´ í† í°: {sum(count_tokens(t) for t in texts)}")
    
    response = requests.post(EMBEDDING_URL, headers=headers, json=data)
    
    if response.status_code != 200:
        print(f"âŒ API ì˜¤ë¥˜: {response.status_code}")
        print(f"   ì‘ë‹µ: {response.text}")
        response.raise_for_status()
    
    result = response.json()
    print(f"âœ… API ì„±ê³µ: {len(result.get('data', []))}ê°œ ì„ë² ë”© ë°˜í™˜")
    
    return [item["embedding"] for item in result["data"]]

# âœ… JSONL ì²˜ë¦¬ ë° ì„ë² ë”©
batch = []
batch_token_sum = 0

with open("api_data.jsonl", "r", encoding="utf-8") as f:
    for line in tqdm(f, desc="Processing"):
        item = json.loads(line.strip())
        title, content = item["title"], item["content"]
        full_text = f"{title}\n{content}"
        token_len = count_tokens(full_text)

        if token_len > TOKEN_LIMIT:
            print(f"âš ï¸ {title} â†’ {token_len} tokens (too long, skipped)")
            continue

        if batch_token_sum + token_len > TOKEN_LIMIT:
            try:
                texts = [x["text"] for x in batch]
                embeddings = get_embeddings_batch(texts)

                # ì²« ë²ˆì§¸ ì„ë² ë”©ìœ¼ë¡œ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
                if faiss_index is None and embeddings:
                    first_emb = embeddings[0]
                    actual_dimension = len(first_emb)
                    print(f"ğŸ”§ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”: {actual_dimension}ì°¨ì›")
                    faiss_index = faiss.IndexFlatL2(actual_dimension)

                for i, emb in enumerate(embeddings):
                    vec = np.array(emb, dtype=np.float32)
                    # print(f"   ì„ë² ë”© {i+1}: {len(vec)}ì°¨ì›")
                    
                    # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
                    faiss_index.add(np.array([vec]))

                    cursor.execute(
                        "INSERT INTO embeddings (title, content, embedding) VALUES (%s, %s, %s)",
                        (batch[i]["title"], batch[i]["content"], json.dumps(emb))
                    )
                    meta_data.append({
                        "title": batch[i]["title"],
                        "content": batch[i]["content"],
                        "embedding": emb
                    })

                conn.commit()
            except Exception as e:
                print("âŒ Batch ì²˜ë¦¬ ì‹¤íŒ¨:", str(e))
                print(f"   ë°°ì¹˜ í¬ê¸°: {len(batch)}")
                print(f"   ë°°ì¹˜ í† í° í•©ê³„: {batch_token_sum}")
                import traceback
                traceback.print_exc()

            # ìƒˆ ë°°ì¹˜ ì´ˆê¸°í™”
            batch = []
            batch_token_sum = 0

        batch.append({
            "title": title,
            "content": content,
            "text": full_text
        })
        batch_token_sum += token_len

# âœ… ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
if batch:
    try:
        texts = [x["text"] for x in batch]
        embeddings = get_embeddings_batch(texts)

        # ì²« ë²ˆì§¸ ì„ë² ë”©ìœ¼ë¡œ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°)
        if faiss_index is None and embeddings:
            first_emb = embeddings[0]
            actual_dimension = len(first_emb)
            print(f"ğŸ”§ FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”: {actual_dimension}ì°¨ì›")
            faiss_index = faiss.IndexFlatL2(actual_dimension)

        for i, emb in enumerate(embeddings):
            vec = np.array(emb, dtype=np.float32)
            print(f"   ì„ë² ë”© {i+1}: {len(vec)}ì°¨ì›")
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            faiss_index.add(np.array([vec]))

            cursor.execute(
                "INSERT INTO embeddings (title, content, embedding) VALUES (%s, %s, %s)",
                (batch[i]["title"], batch[i]["content"], json.dumps(emb))
            )
            meta_data.append({
                "title": batch[i]["title"],
                "content": batch[i]["content"],
                "embedding": emb
            })

        conn.commit()
    except Exception as e:
        print("âŒ ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨:", str(e))
        print(f"   ë°°ì¹˜ í¬ê¸°: {len(batch)}")
        print(f"   ë°°ì¹˜ í† í° í•©ê³„: {batch_token_sum}")
        import traceback
        traceback.print_exc()

cursor.close()
conn.close()

# âœ… ê²°ê³¼ ì €ì¥
os.makedirs("faiss_index", exist_ok=True)
faiss.write_index(faiss_index, "faiss_index/valorant.index")
with open("faiss_index/valorant_meta.json", "w", encoding="utf-8") as f:
    json.dump(meta_data, f, ensure_ascii=False, indent=2)
