import requests
import faiss
import numpy as np
import os
import json
from dotenv import load_dotenv
import tiktoken

load_dotenv()

GMS_KEY = os.getenv("OPENAI_API_KEY")
EMBEDDING_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1/embeddings"
MODEL = "text-embedding-3-large"
MAX_TOTAL_TOKENS = 8191  # OpenAI ëª¨ë¸ ì œí•œ

# ğŸ”¸ í† í¬ë‚˜ì´ì € ì„¤ì •
encoding = tiktoken.encoding_for_model(MODEL)

# ğŸ”½ ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
docs = []
with open("input_docs.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        docs.append(json.loads(line.strip()))

# í…ìŠ¤íŠ¸ ì¶”ì¶œ + í† í° ìˆ˜ ê³„ì‚°
texts = [f"{doc['title']} {doc['context']}" for doc in docs]
text_token_pairs = [(text, len(encoding.encode(text))) for text in texts]

# ğŸ” ë°°ì¹˜ êµ¬ì„± í•¨ìˆ˜ (ì´ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°)
def make_batches(pairs, max_tokens=MAX_TOTAL_TOKENS):
    batches = []
    batch = []
    token_sum = 0

    for text, tokens in pairs:
        if tokens > max_tokens:
            print(f"âš ï¸ ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ë‹¨ë… ì²˜ë¦¬ ì˜ˆì • (tokens={tokens})")
            if batch:
                batches.append(batch)
                batch = []
                token_sum = 0
            batches.append([(text, tokens)])  # ë‹¨ë… ì²˜ë¦¬
            continue

        if token_sum + tokens > max_tokens:
            batches.append(batch)
            batch = [(text, tokens)]
            token_sum = tokens
        else:
            batch.append((text, tokens))
            token_sum += tokens

    if batch:
        batches.append(batch)
    return batches

# ğŸ” ì„ë² ë”© í•¨ìˆ˜
def get_embeddings_batch(text_batch):
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {GMS_KEY}"
    }
    data = {
        "model": MODEL,
        "input": [t for t, _ in text_batch]
    }
    res = requests.post(EMBEDDING_URL, headers=headers, json=data)
    res.raise_for_status()
    return [item["embedding"] for item in res.json()["data"]]

# ğŸ§  ì„ë² ë”© ì‹¤í–‰
embeddings = []
batches = make_batches(text_token_pairs)

print(f"ğŸ”¹ ì´ ë°°ì¹˜ ìˆ˜: {len(batches)}")

for i, batch in enumerate(batches):
    print(f"ğŸ”„ {i+1}/{len(batches)} ë°°ì¹˜ ì„ë² ë”© ì¤‘... (ì´ {sum(t for _, t in batch)} tokens)")
    try:
        embs = get_embeddings_batch(batch)
        embeddings.extend(embs)
        print(f"âœ… ì™„ë£Œ: {len(embs)}ê°œ")
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")

# ğŸ’¾ FAISS ì €ì¥
embeddings = np.array(embeddings).astype("float32")
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

os.makedirs("faiss_index", exist_ok=True)
faiss.write_index(index, "faiss_index/fnatic_large.index")

with open("faiss_index/rag.json", "w", encoding="utf-8") as f:
    json.dump(docs, f, ensure_ascii=False, indent=2)

print("âœ… ì„ë² ë”© ë° ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ (text-embedding-3-large)")
